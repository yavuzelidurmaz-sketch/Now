import cloudscraper
from bs4 import BeautifulSoup
import time
import re
import subprocess
from urllib.parse import urljoin

# --- AYARLAR ---
BASE_URL = "https://www.nowtv.com.tr"
M3U_FILENAME = "nowtv.m3u"

# KATEGORÄ° HARÄ°TASI
CATEGORY_MAP = [
    {"url": "https://www.nowtv.com.tr/dizi-izle", "name": "NOW DIZILER"},
    {"url": "https://www.nowtv.com.tr/program-izle", "name": "NOW PROGRAMLAR"},
    {"url": "https://www.nowtv.com.tr/now-spor", "name": "NOW SPOR"},
    {"url": "https://www.nowtv.com.tr/now-haber", "name": "NOW HABER"},
    {"url": "https://www.nowtv.com.tr/dizi-arsivi", "name": "NOW DIZI ARSIV"},
    {"url": "https://www.nowtv.com.tr/program-arsivi", "name": "NOW PROGRAM ARSIV"}
]

def get_single_m3u8(scraper, url):
    """Eksik kalan tekil sayfalardan m3u8 Ã§eker."""
    try:
        time.sleep(0.3)
        r = scraper.get(url, timeout=10)
        match = re.search(r'https?://[^\s"\'\\,]+\.m3u8[^\s"\'\\,]*', r.text)
        if match:
            return match.group(0).replace('\\/', '/')
        return url
    except:
        return url

def commit_and_push(file_name):
    print(f"\nğŸ“¤ {file_name} GitHub'a gÃ¶nderiliyor...")
    try:
        subprocess.run(["git", "config", "--global", "user.name", "github-actions[bot]"], check=True)
        subprocess.run(["git", "config", "--global", "user.email", "github-actions[bot]@users.noreply.github.com"], check=True)
        subprocess.run(["git", "add", file_name], check=True)
        status = subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True).stdout
        if status:
            subprocess.run(["git", "commit", "-m", f"ğŸ”„ NOW TV M3U Update: {time.strftime('%Y-%m-%d')}"], check=True)
            subprocess.run(["git", "push", "--force"], check=True)
            print("ğŸš€ GitHub'a baÅŸarÄ±yla yÃ¼klendi!")
        else:
            print("âš ï¸ DeÄŸiÅŸiklik yok, push atlanÄ±yor.")
    except Exception as e:
        print(f"âŒ Git HatasÄ±: {e}")

def collect_dynamic_series(scraper):
    """
    Kategorileri tarar ve iÃ§erikleri kategorisine gÃ¶re etiketleyerek listeler.
    YENÄ° YÃ–NTEM: Class baÄŸÄ±msÄ±z, direkt link analizi.
    """
    dynamic_data = {}
    print("ğŸŒ Kategori sayfalarÄ± taranÄ±yor...")

    for cat in CATEGORY_MAP:
        url = cat["url"]
        cat_name = cat["name"]
        print(f"   ğŸ“‚ Kategori TaranÄ±yor: {cat_name} ({url})")
        
        try:
            resp = scraper.get(url, timeout=15)
            # Sayfa gerÃ§ekten yÃ¼klendi mi kontrol et
            if resp.status_code != 200:
                print(f"      âŒ Hata: Sayfa yÃ¼klenemedi (Kod: {resp.status_code})")
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # Sayfa baÅŸlÄ±ÄŸÄ±nÄ± yazdÄ±r (Cloudflare engeli var mÄ± anlamak iÃ§in)
            page_title = soup.title.string.strip() if soup.title else "BaÅŸlÄ±k Yok"
            # print(f"      â„¹ï¸ Sayfa BaÅŸlÄ±ÄŸÄ±: {page_title}") 

            # YENÄ° YÃ–NTEM: TÃ¼m 'a' etiketlerini bul ve link yapÄ±larÄ±na gÃ¶re filtrele
            all_links = soup.find_all('a', href=True)
            
            count_in_cat = 0
            for link_tag in all_links:
                href = link_tag['href']
                full_link = urljoin(BASE_URL, href)
                
                # Gereksiz linkleri ele (iletiÅŸim, kÃ¼nye, kategori ana linkleri vb.)
                if any(x in href for x in ['/iletisim', '/kunye', '/yayin-akisi', 'facebook', 'twitter', 'instagram']):
                    continue
                if full_link.rstrip('/') == url.rstrip('/'): # Kendi linkini alma
                    continue

                # Sadece hedef yapÄ±ya uyan linkleri al
                # Ã–rn: /dizi/kizil-goncalar, /program/cagla-ile-yeni-bir-gun
                is_valid_content = False
                if '/dizi/' in href and '/dizi-izle' not in href and '/dizi-arsivi' not in href:
                    is_valid_content = True
                elif '/program/' in href and '/program-izle' not in href and '/program-arsivi' not in href:
                    is_valid_content = True
                elif '/now-haber' in href or '/now-spor' in href:
                    # Haber ve Spor linkleri genelde ana kategori linkiyle aynÄ± olabiliyor, alt iÃ§erikleri ayÄ±r
                    if len(href.split('/')) > 2: 
                        is_valid_content = True

                if not is_valid_content:
                    continue

                try:
                    # Ä°sim Ã‡Ä±karma (Title > Alt Text > Link Sonu)
                    title = ""
                    img_tag = link_tag.find('img')
                    
                    if link_tag.get('title'):
                        title = link_tag.get('title').strip()
                    elif img_tag and img_tag.get('alt'):
                        title = img_tag.get('alt').strip()
                    elif link_tag.find('span', class_='title'): # Yedek class kontrolÃ¼
                        title = link_tag.find('span', class_='title').get_text(strip=True)
                    else:
                        # Linkten isim Ã¼ret: /dizi/yabani -> Yabani
                        title = href.strip('/').split('/')[-1].replace('-', ' ').title()

                    # Resim Ã‡Ä±karma
                    img_url = ""
                    if img_tag:
                        img_url = img_tag.get('data-src') or img_tag.get('src')
                        if img_url and not img_url.startswith('http'):
                            img_url = urljoin(BASE_URL, img_url)
                    
                    # EÄŸer resim yoksa varsayÄ±lan bir logo koyalÄ±m
                    if not img_url:
                        img_url = "https://img-nowtv.mncdn.com/assets/images/nowtv-logo-share.jpg"

                    # ID (Key) oluÅŸturma
                    dizi_key = href.strip('/').split('/')[-1]

                    # MÃ¼kerrer kayÄ±t Ã¶nleme
                    if dizi_key not in dynamic_data:
                        dynamic_data[dizi_key] = {
                            "isim": title,
                            "link": full_link,
                            "resim": img_url,
                            "kategori": cat_name
                        }
                        count_in_cat += 1
                except:
                    continue
            
            print(f"      âœ… Bu kategoriden {count_in_cat} iÃ§erik eklendi.")

        except Exception as e:
            print(f"   âš ï¸ Hata ({url}): {e}")
    
    print(f"\nğŸŒ Toplam {len(dynamic_data)} farklÄ± iÃ§erik bulundu.\n")
    return dynamic_data

def run_scraper():
    print("ğŸš€ Bot BaÅŸlatÄ±ldÄ±. M3U Modu Aktif...")
    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})
    
    target_series = collect_dynamic_series(scraper)
    
    if not target_series:
        print("âŒ HiÃ§bir iÃ§erik bulunamadÄ±! Site yapÄ±sÄ± deÄŸiÅŸmiÅŸ veya IP engelli olabilir.")
        # Dosya boÅŸ olsa bile eskiyi silmek iÃ§in commit atabiliriz ama ÅŸimdilik durduralÄ±m.
        return

    memory_data = {}

    for dizi_key, info in target_series.items():
        title = info.get('isim', 'Dizi')
        dizi_url = info.get('link', '')
        poster = info.get('resim', '')
        category = info.get('kategori', 'GENEL')
        
        # Link DÃ¼zenleme: Haber/Spor iÃ§in direkt link, diziler iÃ§in /bolumler
        base_series_url = dizi_url.split('/izle')[0].rstrip('/')
        
        # Haber ve Spor sayfalarÄ±nda yapÄ± farklÄ± olabiliyor, direkt tarayalÄ±m
        if "now-haber" in base_series_url or "now-spor" in base_series_url:
            bolumler_url = base_series_url 
        else:
            bolumler_url = base_series_url + "/bolumler"

        print(f"ğŸ” [{category}] {title} iÅŸleniyor...", end=" ", flush=True)
        
        try:
            response = scraper.get(bolumler_url, timeout=10)
            
            # 1. AÅŸama: Sayfa iÃ§indeki tÃ¼m .m3u8 linklerini topla (HÄ±zlÄ± YÃ¶ntem)
            found_m3u8s = re.findall(r'https?://[^\s"\'\\,]+\.m3u8[^\s"\'\\,]*', response.text)
            found_m3u8s = [m.replace('\\/', '/') for m in found_m3u8s]
            unique_m3u8s = list(dict.fromkeys(found_m3u8s))

            b_soup = BeautifulSoup(response.text, 'html.parser')
            eps = []
            
            select_box = b_soup.find('select', id='video-finder-changer')
            
            # --- SENARYO A: Select Box Var (Standart Dizi SayfasÄ±) ---
            if select_box:
                options = select_box.find_all('option', {'data-target': True})
                print(f"({len(options)} BÃ¶lÃ¼m)")
                
                for i, opt in enumerate(options):
                    b_title = opt.get_text(strip=True)
                    b_target = opt['data-target']
                    
                    # Elimizdeki hazÄ±r m3u8 listesinden eÅŸleÅŸtirmeye Ã§alÄ±ÅŸ
                    link = unique_m3u8s[i] if i < len(unique_m3u8s) else b_target
                    
                    # EÄŸer link bir sayfa linkiyse (m3u8 deÄŸilse), iÃ§ine girip al (Deep Scan)
                    if ".m3u8" not in link:
                        if not b_target.startswith('http'):
                            b_target = urljoin(BASE_URL, b_target)
                        # Ã‡ok yavaÅŸlamamasÄ± iÃ§in sadece ilk ve son bÃ¶lÃ¼mlerde deep scan yapabilirsin
                        # Ama tam liste iÃ§in mecbur hepsine bakacaÄŸÄ±z:
                        link = get_single_m3u8(scraper, b_target)
                    
                    eps.append({"ad": b_title, "link": link})
            
            # --- SENARYO B: Select Box Yok (Video KartlarÄ± / Haber / Spor) ---
            else:
                # Video kartlarÄ±nÄ± bul (GeniÅŸ selector)
                video_cards = b_soup.select('.video-item, .grid-item, .col-md-4 a, .card-video a')
                
                # EÄŸer direkt video kartÄ± bulamadÄ±ysa, sayfadaki tÃ¼m 'izle' linklerine bak
                if not video_cards:
                     all_links = b_soup.find_all('a', href=True)
                     video_cards = [l for l in all_links if '/izle/' in l['href']]

                if video_cards:
                     print(f"({len(video_cards)} Video - Alternatif)")
                     count = 0
                     for card in video_cards:
                         if count >= 20: break # Ã‡ok fazla video varsa limitle (haberler vb.)
                         
                         v_link = card.get('href')
                         # BaÅŸlÄ±k bulma Ã§abasÄ±
                         v_title = card.get('title') 
                         if not v_title: 
                             v_title = card.find('img')['alt'] if card.find('img') else card.get_text(strip=True)
                         if not v_title:
                             v_title = "Video"

                         if v_link and "/izle/" in v_link:
                             full_v_link = urljoin(BASE_URL, v_link)
                             # Link ana dizi linkiyle aynÄ±ysa atla
                             if full_v_link.rstrip('/') == base_series_url.rstrip('/'): continue

                             link = get_single_m3u8(scraper, full_v_link)
                             eps.append({"ad": v_title, "link": link})
                             count += 1
                else:
                    print("(BÃ¶lÃ¼m bulunamadÄ±)")

            if eps:
                memory_data[dizi_key] = {
                    "isim": title, 
                    "resim": poster, 
                    "kategori": category,
                    "bolumler": eps
                }
            else:
                # BÃ¶lÃ¼m bulamasa bile 'CanlÄ± YayÄ±n' veya tekil iÃ§erik olabilir mi?
                # Åimdilik boÅŸ geÃ§iyoruz.
                pass

        except Exception as e:
            print(f"âš ï¸ Hata: {e}")

    if memory_data:
        create_m3u(memory_data)
    else:
        print("âŒ M3U oluÅŸturulacak veri yok.")

def create_m3u(data):
    print(f"\nğŸ“ {M3U_FILENAME} dosyasÄ± oluÅŸturuluyor...")
    
    with open(M3U_FILENAME, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        
        # Kategorilere gÃ¶re sÄ±rala
        sorted_keys = sorted(data.keys(), key=lambda k: data[k]['kategori'])
        
        for key in sorted_keys:
            item = data[key]
            group = item['kategori']
            poster = item['resim']
            series_name = item['isim']
            
            for bolum in item['bolumler']:
                ep_name = bolum['ad']
                link = bolum['link']
                
                # M3U SatÄ±r FormatÄ±
                f.write(f'#EXTINF:-1 group-title="{group}" tvg-logo="{poster}", {series_name} - {ep_name}\n')
                f.write(f'{link}\n')

    commit_and_push(M3U_FILENAME)

if __name__ == "__main__":
    run_scraper()
